{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK4WVjYki9UAaTrxIWg4OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeminboo/25-1-SelfDriving/blob/main/02_week4_DQN/02_week4_DQN_%EB%B6%80%ED%98%9C%EB%AF%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "세 개의 레버가 달린 장치와 원숭이가 있다.\n",
        "\n",
        "장치의 첫 번째 레버를 당기면 쓴 약이, 세 번째 레버를 당기면 바나나가 나온다.\n",
        "\n",
        "한 번 당긴 레버는 자동으로 다시 올라간다고 가정할 때, 이 장치와 원숭이를 코드로 추상화시키고, DQN을 이용해 학습시켜라."
      ],
      "metadata": {
        "id": "x-fG_l7ZAlc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QBHS4M0Y_sEm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeverEnv(gym.Env): # Gym의 Env를 기반으로 환경 만듦\n",
        "    def __init__(self):\n",
        "        super(LeverEnv, self).__init__()\n",
        "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        return np.array([0.0], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            reward = -100\n",
        "        elif action == 1:\n",
        "            reward = 10\n",
        "        else:\n",
        "            reward = 100\n",
        "        done = True\n",
        "        return np.array([0.0], dtype=np.float32), reward, done, {} # [0.0] : dummy state (원숭이가 행동을 결정할 때 어떤 상태 정보도 필요하지 않기 때문)"
      ],
      "metadata": {
        "id": "A4napvUPA_0R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 3) # 액션 수 만큼 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        ""
      ],
      "metadata": {
        "id": "ybb3_QjjDICa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, maxlen=1000):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # 경험 하나를 버퍼에 저장\n",
        "        self.buffer.append((state, int(action), reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # 랜덤 샘플링\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        states = torch.FloatTensor([b[0] for b in batch])\n",
        "        actions = torch.LongTensor([b[1] for b in batch])\n",
        "        rewards = torch.FloatTensor([b[2] for b in batch])\n",
        "        next_states = torch.FloatTensor([b[3] for b in batch])\n",
        "        dones = torch.FloatTensor([float(b[4]) for b in batch])\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "brxmTuDbEX0B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    env = LeverEnv()\n",
        "    qnet = QNetwork()\n",
        "    target_net = QNetwork()\n",
        "    target_net.load_state_dict(qnet.state_dict())\n",
        "    target_net.eval() # target network는 학습하지 않음\n",
        "\n",
        "    buffer = ReplayBuffer()\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=0.001)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    episodes = 300\n",
        "    batch_size = 32\n",
        "    gamma = 0.9\n",
        "    epsilon = 0.1\n",
        "    update_target = 20\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_vals = qnet(torch.FloatTensor(state)) # state의 q 값 예측\n",
        "                    action = torch.argmax(q_vals).item() # 가장 큰 q 값의 액션 선택\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "\n",
        "                q_values = qnet(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    next_q_values = target_net(s_).max(1)[0]\n",
        "                    target = r + gamma * next_q_values * (1-d)\n",
        "\n",
        "                loss = loss_fn(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if (episode + 1) % update_target == 0:\n",
        "            target_net.load_state_dict(qnet.state_dict())\n",
        "\n",
        "        print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return qnet"
      ],
      "metadata": {
        "id": "YlZLnNDfE5_d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(qnet):\n",
        "    env = LeverEnv()\n",
        "    for a in range(3):\n",
        "        with torch.no_grad():\n",
        "            q_val = qnet(torch.FloatTensor([0]))[a].item()\n",
        "            print(f\"Lever {a}: Q value = {q_val:.2f}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    with torch.no_grad():\n",
        "        action = torch.argmax(qnet(torch.FloatTensor(state))).item()\n",
        "    print(f\"Lever {action}을 당김\")"
      ],
      "metadata": {
        "id": "cvtZGeMyIKBo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qnet = train()\n",
        "test(qnet)\n",
        "\n",
        "# reward : -1, 0, 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ImG2C1cIxjE",
        "outputId": "c4725350-018b-44ce-b111-62fc7a8ec538"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-8ff51a07ca26>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.buffer.append((state, int(action), reward, next_state, done))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20, Total Reward: 1\n",
            "Episode 40, Total Reward: 1\n",
            "Episode 60, Total Reward: 1\n",
            "Episode 80, Total Reward: 1\n",
            "Episode 100, Total Reward: 1\n",
            "Episode 120, Total Reward: 1\n",
            "Episode 140, Total Reward: 1\n",
            "Episode 160, Total Reward: 1\n",
            "Episode 180, Total Reward: 1\n",
            "Episode 200, Total Reward: 1\n",
            "Episode 220, Total Reward: 1\n",
            "Episode 240, Total Reward: 1\n",
            "Episode 260, Total Reward: 1\n",
            "Episode 280, Total Reward: 1\n",
            "Episode 300, Total Reward: 1\n",
            "Lever 0: Q value = 1.00\n",
            "Lever 1: Q value = 0.03\n",
            "Lever 2: Q value = 1.00\n",
            "Lever 2을 당김\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qnet = train()\n",
        "test(qnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aTyQWwVKNbz",
        "outputId": "15d579e8-513e-4938-9288-ff4fdec6cbef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -100\n",
            "Episode 2, Total Reward: -100\n",
            "Episode 3, Total Reward: 100\n",
            "Episode 4, Total Reward: -100\n",
            "Episode 5, Total Reward: -100\n",
            "Episode 6, Total Reward: -100\n",
            "Episode 7, Total Reward: -100\n",
            "Episode 8, Total Reward: -100\n",
            "Episode 9, Total Reward: 100\n",
            "Episode 10, Total Reward: -100\n",
            "Episode 11, Total Reward: -100\n",
            "Episode 12, Total Reward: -100\n",
            "Episode 13, Total Reward: -100\n",
            "Episode 14, Total Reward: -100\n",
            "Episode 15, Total Reward: -100\n",
            "Episode 16, Total Reward: -100\n",
            "Episode 17, Total Reward: -100\n",
            "Episode 18, Total Reward: -100\n",
            "Episode 19, Total Reward: -100\n",
            "Episode 20, Total Reward: -100\n",
            "Episode 21, Total Reward: 100\n",
            "Episode 22, Total Reward: -100\n",
            "Episode 23, Total Reward: -100\n",
            "Episode 24, Total Reward: 100\n",
            "Episode 25, Total Reward: -100\n",
            "Episode 26, Total Reward: -100\n",
            "Episode 27, Total Reward: -100\n",
            "Episode 28, Total Reward: 100\n",
            "Episode 29, Total Reward: -100\n",
            "Episode 30, Total Reward: -100\n",
            "Episode 31, Total Reward: -100\n",
            "Episode 32, Total Reward: -100\n",
            "Episode 33, Total Reward: 100\n",
            "Episode 34, Total Reward: 100\n",
            "Episode 35, Total Reward: 100\n",
            "Episode 36, Total Reward: 100\n",
            "Episode 37, Total Reward: 100\n",
            "Episode 38, Total Reward: 100\n",
            "Episode 39, Total Reward: 100\n",
            "Episode 40, Total Reward: 100\n",
            "Episode 41, Total Reward: 100\n",
            "Episode 42, Total Reward: 100\n",
            "Episode 43, Total Reward: 100\n",
            "Episode 44, Total Reward: 100\n",
            "Episode 45, Total Reward: 100\n",
            "Episode 46, Total Reward: 100\n",
            "Episode 47, Total Reward: 100\n",
            "Episode 48, Total Reward: 100\n",
            "Episode 49, Total Reward: 100\n",
            "Episode 50, Total Reward: 100\n",
            "Episode 51, Total Reward: 100\n",
            "Episode 52, Total Reward: 100\n",
            "Episode 53, Total Reward: 100\n",
            "Episode 54, Total Reward: 100\n",
            "Episode 55, Total Reward: 100\n",
            "Episode 56, Total Reward: 100\n",
            "Episode 57, Total Reward: 100\n",
            "Episode 58, Total Reward: 100\n",
            "Episode 59, Total Reward: 100\n",
            "Episode 60, Total Reward: 100\n",
            "Episode 61, Total Reward: 100\n",
            "Episode 62, Total Reward: 100\n",
            "Episode 63, Total Reward: 100\n",
            "Episode 64, Total Reward: 100\n",
            "Episode 65, Total Reward: 100\n",
            "Episode 66, Total Reward: 100\n",
            "Episode 67, Total Reward: 100\n",
            "Episode 68, Total Reward: 100\n",
            "Episode 69, Total Reward: 100\n",
            "Episode 70, Total Reward: 100\n",
            "Episode 71, Total Reward: 100\n",
            "Episode 72, Total Reward: 100\n",
            "Episode 73, Total Reward: 100\n",
            "Episode 74, Total Reward: 100\n",
            "Episode 75, Total Reward: 100\n",
            "Episode 76, Total Reward: 100\n",
            "Episode 77, Total Reward: 100\n",
            "Episode 78, Total Reward: 100\n",
            "Episode 79, Total Reward: 100\n",
            "Episode 80, Total Reward: 100\n",
            "Episode 81, Total Reward: 100\n",
            "Episode 82, Total Reward: 100\n",
            "Episode 83, Total Reward: 100\n",
            "Episode 84, Total Reward: 100\n",
            "Episode 85, Total Reward: 100\n",
            "Episode 86, Total Reward: 100\n",
            "Episode 87, Total Reward: 100\n",
            "Episode 88, Total Reward: 100\n",
            "Episode 89, Total Reward: 100\n",
            "Episode 90, Total Reward: 100\n",
            "Episode 91, Total Reward: 100\n",
            "Episode 92, Total Reward: 100\n",
            "Episode 93, Total Reward: 100\n",
            "Episode 94, Total Reward: 100\n",
            "Episode 95, Total Reward: 100\n",
            "Episode 96, Total Reward: 100\n",
            "Episode 97, Total Reward: 100\n",
            "Episode 98, Total Reward: 100\n",
            "Episode 99, Total Reward: 100\n",
            "Episode 100, Total Reward: 100\n",
            "Episode 101, Total Reward: 100\n",
            "Episode 102, Total Reward: 100\n",
            "Episode 103, Total Reward: 100\n",
            "Episode 104, Total Reward: 100\n",
            "Episode 105, Total Reward: 100\n",
            "Episode 106, Total Reward: 100\n",
            "Episode 107, Total Reward: 100\n",
            "Episode 108, Total Reward: 100\n",
            "Episode 109, Total Reward: 100\n",
            "Episode 110, Total Reward: 100\n",
            "Episode 111, Total Reward: 100\n",
            "Episode 112, Total Reward: 100\n",
            "Episode 113, Total Reward: 100\n",
            "Episode 114, Total Reward: 100\n",
            "Episode 115, Total Reward: 100\n",
            "Episode 116, Total Reward: 100\n",
            "Episode 117, Total Reward: 100\n",
            "Episode 118, Total Reward: 100\n",
            "Episode 119, Total Reward: 100\n",
            "Episode 120, Total Reward: 100\n",
            "Episode 121, Total Reward: 100\n",
            "Episode 122, Total Reward: 100\n",
            "Episode 123, Total Reward: 100\n",
            "Episode 124, Total Reward: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-8ff51a07ca26>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.buffer.append((state, int(action), reward, next_state, done))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 125, Total Reward: 100\n",
            "Episode 126, Total Reward: 100\n",
            "Episode 127, Total Reward: 100\n",
            "Episode 128, Total Reward: 100\n",
            "Episode 129, Total Reward: 100\n",
            "Episode 130, Total Reward: 100\n",
            "Episode 131, Total Reward: 100\n",
            "Episode 132, Total Reward: 100\n",
            "Episode 133, Total Reward: 100\n",
            "Episode 134, Total Reward: 100\n",
            "Episode 135, Total Reward: 100\n",
            "Episode 136, Total Reward: 100\n",
            "Episode 137, Total Reward: 100\n",
            "Episode 138, Total Reward: 100\n",
            "Episode 139, Total Reward: 100\n",
            "Episode 140, Total Reward: 100\n",
            "Episode 141, Total Reward: 100\n",
            "Episode 142, Total Reward: 100\n",
            "Episode 143, Total Reward: 100\n",
            "Episode 144, Total Reward: 100\n",
            "Episode 145, Total Reward: 100\n",
            "Episode 146, Total Reward: 100\n",
            "Episode 147, Total Reward: 100\n",
            "Episode 148, Total Reward: 100\n",
            "Episode 149, Total Reward: 100\n",
            "Episode 150, Total Reward: 100\n",
            "Episode 151, Total Reward: 100\n",
            "Episode 152, Total Reward: 100\n",
            "Episode 153, Total Reward: 100\n",
            "Episode 154, Total Reward: 100\n",
            "Episode 155, Total Reward: 100\n",
            "Episode 156, Total Reward: 100\n",
            "Episode 157, Total Reward: 100\n",
            "Episode 158, Total Reward: 100\n",
            "Episode 159, Total Reward: 100\n",
            "Episode 160, Total Reward: 100\n",
            "Episode 161, Total Reward: 100\n",
            "Episode 162, Total Reward: 100\n",
            "Episode 163, Total Reward: 100\n",
            "Episode 164, Total Reward: 100\n",
            "Episode 165, Total Reward: 100\n",
            "Episode 166, Total Reward: 100\n",
            "Episode 167, Total Reward: 100\n",
            "Episode 168, Total Reward: 100\n",
            "Episode 169, Total Reward: 100\n",
            "Episode 170, Total Reward: 100\n",
            "Episode 171, Total Reward: 100\n",
            "Episode 172, Total Reward: 100\n",
            "Episode 173, Total Reward: 100\n",
            "Episode 174, Total Reward: 100\n",
            "Episode 175, Total Reward: 100\n",
            "Episode 176, Total Reward: 100\n",
            "Episode 177, Total Reward: 100\n",
            "Episode 178, Total Reward: 100\n",
            "Episode 179, Total Reward: 100\n",
            "Episode 180, Total Reward: 100\n",
            "Episode 181, Total Reward: 100\n",
            "Episode 182, Total Reward: 100\n",
            "Episode 183, Total Reward: 100\n",
            "Episode 184, Total Reward: 100\n",
            "Episode 185, Total Reward: 100\n",
            "Episode 186, Total Reward: 100\n",
            "Episode 187, Total Reward: 100\n",
            "Episode 188, Total Reward: 100\n",
            "Episode 189, Total Reward: 100\n",
            "Episode 190, Total Reward: 100\n",
            "Episode 191, Total Reward: 100\n",
            "Episode 192, Total Reward: 100\n",
            "Episode 193, Total Reward: 100\n",
            "Episode 194, Total Reward: 100\n",
            "Episode 195, Total Reward: 100\n",
            "Episode 196, Total Reward: 100\n",
            "Episode 197, Total Reward: 100\n",
            "Episode 198, Total Reward: 100\n",
            "Episode 199, Total Reward: 100\n",
            "Episode 200, Total Reward: 100\n",
            "Episode 201, Total Reward: 100\n",
            "Episode 202, Total Reward: 100\n",
            "Episode 203, Total Reward: 100\n",
            "Episode 204, Total Reward: 100\n",
            "Episode 205, Total Reward: 100\n",
            "Episode 206, Total Reward: 100\n",
            "Episode 207, Total Reward: 100\n",
            "Episode 208, Total Reward: 100\n",
            "Episode 209, Total Reward: 100\n",
            "Episode 210, Total Reward: 100\n",
            "Episode 211, Total Reward: 100\n",
            "Episode 212, Total Reward: 100\n",
            "Episode 213, Total Reward: 100\n",
            "Episode 214, Total Reward: 100\n",
            "Episode 215, Total Reward: 100\n",
            "Episode 216, Total Reward: 100\n",
            "Episode 217, Total Reward: 100\n",
            "Episode 218, Total Reward: 100\n",
            "Episode 219, Total Reward: 100\n",
            "Episode 220, Total Reward: 100\n",
            "Episode 221, Total Reward: 100\n",
            "Episode 222, Total Reward: 100\n",
            "Episode 223, Total Reward: 100\n",
            "Episode 224, Total Reward: 100\n",
            "Episode 225, Total Reward: 100\n",
            "Episode 226, Total Reward: 100\n",
            "Episode 227, Total Reward: 100\n",
            "Episode 228, Total Reward: 100\n",
            "Episode 229, Total Reward: 100\n",
            "Episode 230, Total Reward: 100\n",
            "Episode 231, Total Reward: 100\n",
            "Episode 232, Total Reward: 100\n",
            "Episode 233, Total Reward: 100\n",
            "Episode 234, Total Reward: 100\n",
            "Episode 235, Total Reward: 100\n",
            "Episode 236, Total Reward: 100\n",
            "Episode 237, Total Reward: 100\n",
            "Episode 238, Total Reward: 100\n",
            "Episode 239, Total Reward: 100\n",
            "Episode 240, Total Reward: 100\n",
            "Episode 241, Total Reward: 100\n",
            "Episode 242, Total Reward: 100\n",
            "Episode 243, Total Reward: 100\n",
            "Episode 244, Total Reward: 100\n",
            "Episode 245, Total Reward: 100\n",
            "Episode 246, Total Reward: 100\n",
            "Episode 247, Total Reward: 100\n",
            "Episode 248, Total Reward: 100\n",
            "Episode 249, Total Reward: 100\n",
            "Episode 250, Total Reward: 100\n",
            "Episode 251, Total Reward: 100\n",
            "Episode 252, Total Reward: 100\n",
            "Episode 253, Total Reward: 100\n",
            "Episode 254, Total Reward: 100\n",
            "Episode 255, Total Reward: 100\n",
            "Episode 256, Total Reward: 100\n",
            "Episode 257, Total Reward: 100\n",
            "Episode 258, Total Reward: 100\n",
            "Episode 259, Total Reward: 100\n",
            "Episode 260, Total Reward: 100\n",
            "Episode 261, Total Reward: 100\n",
            "Episode 262, Total Reward: 100\n",
            "Episode 263, Total Reward: 100\n",
            "Episode 264, Total Reward: 100\n",
            "Episode 265, Total Reward: 100\n",
            "Episode 266, Total Reward: 100\n",
            "Episode 267, Total Reward: 100\n",
            "Episode 268, Total Reward: 100\n",
            "Episode 269, Total Reward: 100\n",
            "Episode 270, Total Reward: 100\n",
            "Episode 271, Total Reward: 100\n",
            "Episode 272, Total Reward: 100\n",
            "Episode 273, Total Reward: 100\n",
            "Episode 274, Total Reward: 100\n",
            "Episode 275, Total Reward: 100\n",
            "Episode 276, Total Reward: 100\n",
            "Episode 277, Total Reward: 100\n",
            "Episode 278, Total Reward: 100\n",
            "Episode 279, Total Reward: 100\n",
            "Episode 280, Total Reward: 100\n",
            "Episode 281, Total Reward: 100\n",
            "Episode 282, Total Reward: 100\n",
            "Episode 283, Total Reward: 100\n",
            "Episode 284, Total Reward: 100\n",
            "Episode 285, Total Reward: 100\n",
            "Episode 286, Total Reward: 100\n",
            "Episode 287, Total Reward: 100\n",
            "Episode 288, Total Reward: 100\n",
            "Episode 289, Total Reward: 100\n",
            "Episode 290, Total Reward: 100\n",
            "Episode 291, Total Reward: 100\n",
            "Episode 292, Total Reward: 100\n",
            "Episode 293, Total Reward: 100\n",
            "Episode 294, Total Reward: 100\n",
            "Episode 295, Total Reward: 100\n",
            "Episode 296, Total Reward: 100\n",
            "Episode 297, Total Reward: 100\n",
            "Episode 298, Total Reward: 100\n",
            "Episode 299, Total Reward: 100\n",
            "Episode 300, Total Reward: 100\n",
            "Lever 0: Q value = -0.74\n",
            "Lever 1: Q value = -0.67\n",
            "Lever 2: Q value = 2.88\n",
            "Lever 2을 당김\n"
          ]
        }
      ]
    }
  ]
}